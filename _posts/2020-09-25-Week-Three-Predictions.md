---
title: "Week 3 Analysis: Polling Data"
date: 2020-09-25
---

## Week 3 Analysis: Polling Data
*Friday, September 25, 2020*

This week, I will analyze polling data at the national and state levels to determine if they are good indicators of presidential election outcomes. 

### Assumptions and Theoretical Background
The 1936 *Literary Digest* poll was perhaps one of the earliest well-known examples of [American polling failures](https://ebookcentral-proquest-com.ezp-prod1.hul.harvard.edu/lib/harvard-ebooks/detail.action?docID=6270706). Although the poll gathered an impressive 2.3 million responses, it incorrectly predicted a landslide victory for Republican challenger Alf Landon, estimating that he would carry 33 states and 370 electoral votes. Instead, incumbent Franklin Roosevelt enjoyed an overwhelming landslide, capturing 523 (out of 531) electoral votes and every state except Maine and Vermont.

**Polls are rarely, if ever, perfect. Nonetheless, they can be informative.** As statistician Francis Galton [observed](https://www.nature.com/articles/075450a0) at a livestock exhibition in 1907, having a large sample of people allows individual-level prediction errors to be cancelled out, thus approximating a normal distribution and providing a prediction that is close to the true value - in Galton's case, the weight of an animal. This concept can be applied to polls: the more people that are surveyed about their preferences, the more likely that the poll is to make an accurate prediction about the winner of the election. The idea of poll aggregation also follows the same line of reasoning: with more polls from a variety of sources, errors in individual polls can cancel each other out, and should hypothetically provide a close prediction of the election outcome.

This, of course, leaves much to be desired. The media generally reports on individual polls as they are released, so which polls should the public trust? Do some polls consistently favor one major political party over the other? Or, are polls even accurate at all for predicting election outcomes? I will be exploring some of these questions below.

### Poll Quality
Polls vary in quality based on their sample size and method of sampling, while pollsters vary in quality based on their past polls and their tendency to be more favorable to one of the two major political parties. However, how much do polls vary in quality? Do polls generally under- or over-state the expected vote share of either the Democratic or Republican parties?

To answer these questions, I used individual poll data from the 2016 presidential election (`poll_2016.csv`). Combining the data with [pollster ratings](https://github.com/fivethirtyeight/data/tree/master/pollster-ratings) (`pollster_ratings_538.csv`) from [FiveThirtyEight](https://fivethirtyeight.com/), a popular aggregator of political (and other) polls, I analyzed the residual vote shares of the polls, as defined by `residual = actual - predicted`. This means that a positive residual represents an under-prediction of support (i.e. the poll predicted less support than actually received), while a negative residual represents an over-prediction of support (i.e. the poll predicted more support than actually received). I also separated the predicted votes for Clinton and Trump to see if there is a considerable difference in accurately predicting the vote share between the two major parties.

![Poll Residuals by Party, Based on FiveThirtyEight Poll Grade](https://yanxifang.github.io/Gov-1347/images/resid_twoparty.png)

There are several interesting observations here. First, **the different "grades" of pollsters, as assigned by FiveThirtyEight, do not seem to show major differences in prediction accuracy**. This is surprising because the FiveThirtyEight website [outlines](https://projects.fivethirtyeight.com/pollster-ratings/) a complex series of quantitative steps taken to assign a letter grade to a pollster. While this graph is limited by the fact that it covers only one presidential election (2016), and does not consider the timeframes of polls (averaging polls with equal weight instead), it still raises the question of whether there are significant quality differences between different pollsters. Second, the **polls are much more consistent for the Democratic candidate**: the average residuals are between `0` and `+5` percentage points for all grades of pollsters, compared to `0` and `+10` percentage points for the Republican candidate. This could perhaps indicate an overall bias in polls against the Republican candidate, which would be consistent with two scenarios: (a) the number of Democratic-leaning pollsters is higher than the number of Republican-leaning pollsters, and/or (b) Republican voters have a higher rate of non-response to polls. The latter explanation would seem to validate the concept of the ["silent majority"](https://www.npr.org/2016/01/22/463884201/trump-champions-the-silent-majority-but-what-does-that-mean-in-2016), originally championed by Richard Nixon in 1969 but re-iterated by Trump during his 2016 campaign. The final observation is more of a fluke: **at least in 2016, the pollsters with a grade of "C" actually predicted, on average, the election outcomes very closely**. While individual predictions in the "C" category have a large range, the fact that they balanced each other out (and have symmetric-looking first/third quartiles) is interesting because of the striking parallel to Galton's observations in 1907.

Unfortunately, in the dataset that I have for 2020 election polls (`poll_2020.csv`), the pollster name is not an included variable, so I am unable to build a regression model for 2020 using pollster rating as a variable. Instead, I will take a closer look at state-level polls as an alternative.

### State-Level Polls
Ultimately, national-level polls are only interesting in the sense that they provide an approximate indication of the level of support that each of the two major-party candidates have. Due to the setup of the Electoral College, these **national-level polls are generally not informative about whether a candidate will capture enough electoral votes for an actual victory**; an examination of state-by-state predictions is required. So, the next question is: are state-level polls accurate?

As it turns out, the state-level polls, on average, have been quite accurate. Using the (`pollavg_bystate_1968-2016.csv`) dataset, I found that from 1972 to 2016, across all 50 states (and the District of Columbia), **a given poll's predicted outcome has been the most accurate indicator of the state-level election outcome.** With just this one variable, the regression equation was 

`% of State-Level Popular Vote = 5.6892 + 0.9594(Poll Prediction of Popular Vote)`

This came with a relatively high R-squared value of `0.7457` and a very miniscule p-value. There is little effect on this equation when adding a second variable, the state, which suggests that polls, on average, do not consistently over- or under-estimate the amount of support in any of the 50 states. The third variable I tested was the proximity of each poll to the election (as measured in either days or weeks); while this proximity variable had a statistically significant coefficient, the coefficient was very small.

Below is a scatterplot with points representing each individual poll in my dataset; as previously stated, these polls range in date between 1972 and 2016. The blue points are poll data for the Democratic candidate, and the red points are for the Republican candidate, while the solid black line is the one-variable regression equation listed above. The dashed black line is the 45-degree line, serving as a reference point for the "perfect" poll, where the poll prediction is exactly equal to the election result.

![State Poll Predictions versus Received Vote Share, 1972-2016](https://yanxifang.github.io/Gov-1347/images/state_polls_by_party.png)

As shown above, the overwhelming trend is that the majority of polls tend to be clustered above the 45-degree line; this is reinforced by the fact that the regression line is above the dashed line. This means that **polls generally over-predict the percentage of the popular vote that candidates receive, implying that polls should be revised downward in predicting election outcomes.** In addition, there appears to be a slightly uneven distribution between the Democratic predictions and the Republican predictions; more blue-colored dots appear to be closer to the regression line than red-colored dots. So, I decided to separate the poll predictions for the two parties, as shown below:

![Democratic versus Republican State Polls, 1972-2016](https://yanxifang.github.io/Gov-1347/images/state_poll_twoparty.png)

(Important note: the x-scales do not match up between the two plots, so the comparison may be a bit off.) As seen above, there does not seem to be a major difference between the polling, so for the sake of simplicity, I will ignore political party as an additional variable. Although I previously observed greater inaccuracy for Republican candidate predictions, it seems that those differences are smoothed out on average.

### Prediction
Before coming to the prediction, I wanted to explore one other consideration: the idea that polls converge closer to the actual outcome as the election date approaches. I considered this when selecting variables, but I want to revisit this in a different way because it is a theme present in the literature. As outlined by [Gelman and King (1993)](https://www.cambridge.org/core/journals/british-journal-of-political-science/article/why-are-american-presidential-election-campaign-polls-so-variable-when-votes-are-so-predictable/7936B534442ECC90D60934A450721E8F), voters may inherently have "enlightened preferences" that they eventually translate into a candidate choice. However, the voter does not have a choice until they have gathered sufficient information through the media or the campaigns. Since many voters wait until the last minute to gather most of the information, it follows that polls conducted closest to Election Day are most accurate.

In conducting this "proximity" analysis, I split the dataset into two halves, based on the median "recent-ness" of the polls (45 days was the cut-off). I found that the group of polls that were conducted farther away from the election date (more than 45 days before Election Day) had a R-squared value of `0.6473`, with an equation of: 

`% of State Level Popular Vote = 8.3623 + 0.9059(Poll Prediction of Popular Vote`

However, the group of polls conducted close to Election Day (within 45 days) had a much higher regression R-squared value of `0.8346`, and a very good equation:

`% of State Level Popular Vote = 3.2889 + 1.0060(Poll Prediction of Popular Vote)`

This second equation is particularly notable because the slope is very close to 1. 1 is the slope of the 45-degree line in the two plots above, which would mean that apart from the `3.2889` intercept, the polls would (almost) perfectly predict the outcome!

Now, it's time for the prediction. Since my dataset for 2020 polls (`poll_2020.csv`) only includes polls up to the end of July (about 100 days before the election), I decided to use some creativity in constructing my model. As I did for the historical data, I split the dataset into halves along the median (169 days before November 3, 2020). I then averaged the poll predictions for each state in the dataset (only 42 in the more recent half, and 39 in the less recent half). I then predicted values based on the two regression equations listed above: the "less-recent" equation for the "less-recent" poll averages, and the "more-recent" equation for the "more-recent" poll averages. I then weighted these two results with a simple formula based on the R-squared values of each: taking the R-squared value of the dataset in question, and dividing it by the sum of the R-squared values of both datasets. (This resulted in a weight of `56.32%` for the more recent poll averages, and `43.68%` for the less recent poll averages.) While this is a seemingly unscientific approach, my dataset was also imperfect, with the median being 169 days (compared to the median of 45 in the dataset I used to compute the regression equations).

After applying the weight, I calculated the number of electoral votes (EVs) won by each candidate by tallying the states won. **The model predicts a reasonably strong Biden victory in the 2020 election.** Although my dataset was incomplete (not all 50 states were represented), there was, fortunately, an interesting result: the model predicted at least `301 EVs` for Biden and `154 EVs` for Trump; I am writing "at least" because there are a total of `83 EVs` that belong to states not in my dataset, so these can be won by either candidate. The result is visualized below on a map of the United States.

![Poll Predicted Election Results for 2020](https://yanxifang.github.io/Gov-1347/images/poll_predicted_winner.png)

### Conclusions
This week, I've discovered that state-level polls, particularly those that are conducted close to Election Day, are quite informative and useful in predicting election outcomes. However, while polls may be predictive for past elections, they are far from perfect. 

For one, it's helpful to have an accurate prediction as far away from the election as possible, as discussed last week in the "fundamentals" analysis; unfortunately, polls that are conducted a number of months before the election are not nearly as accurate. Second, the practice of polling may become threatened as the country moves toward alternative forms of communication: with an increasing number of scam calls, the group of American people who are willing to pick up the phone for an unknown number will gradually become less representative of the voting American public. Additionally, the opt-in/opt-out model may further challenge poll accuracy: the longstanding practice of conducting polls by telephone, which disincentivizes people from hanging up on a live human, is increasingly being replaced with online polling, which may be less accurate depending on the types of websites that the polls are placed on, and whether or not people will ignore the poll altogether. Finally, the very rise of extreme partisanship may discourage honest answers. 

All of these challenges threaten the accuracy of polls, as well as the institution of polling as a whole. This means that although polls may be a good predictor of election outcomes today, I strongly believe that political scientists (and perhaps the media) will need to either (a) enhance polling methods for the digital world, or (b) find other variables that provide strong predictions. Without these measures, the public will feel increasingly misled, undermining faith not only in the field of political science, but also in American democracy (e.g. voters can feel discouraged and suspicious when they see election results that are drastically different from the predictions they saw in the media, particularly when there is a candidate they strongly support).

Click [here](https://yanxifang.github.io/Gov-1347) to return to the front page.
